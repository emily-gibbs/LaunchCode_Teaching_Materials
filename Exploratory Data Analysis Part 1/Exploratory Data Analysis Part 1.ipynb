{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d256d04-52b4-4c2b-89f7-019eb040f421",
   "metadata": {},
   "source": [
    "* \"Exploratory Data Analysis (EDA) is understanding the data sets by summarizing their main characteristics often plotting them visually\"\n",
    "* There is no one set method for performing EDA, it depends on the data that you are working with. But there are some common methods and practices\n",
    "* Important libraries: pandas, matplotlib, seaborn\n",
    "* Steps in EDA\n",
    "    - Read in the data! Will usually use pd.read_csv('path_to_csv.csv')\n",
    "    - How much data do you even have? How many rows and columns are there? Can use df.shape\n",
    "        * shows number of rows first and then the number of columns\n",
    "    - What is the background of this data?\n",
    "        * Why was it collected? Who collected it? \n",
    "    - What are the columns and what do they mean? (can help to use df.head and also look back at any documentation on the dataset)\n",
    "        * Can use df.columns or df.info() to list the columns as well\n",
    "    - What data types are contained in each of the columns? (can help to use df.types)\n",
    "        * In addition to coding types (e.g. int, float, bool, etc.) you'll want to identify if they are categorical or numerical, and if they are numerical if they are discrete or continuous. \n",
    "        * Can also use df.info() to see the types\n",
    "    - Based upon the understanding you have gotten of the columns, could there be better more understandable column names? If so it could be a good idea to change their names (can use df.rename(columns = {old_name:new_name, ...}\n",
    "    - Now that you understand the data, this could be a good time to check in and make sure that this is really the data that will help you answer the question you are doing analysis for \n",
    "    - Do you need all of these columns for the work that you are doing? If not, drop the ones you don't need early on so that they aren't taking up space, slowing down your commands, and adding any confusion (use df.drop([column_names], axis=1)\n",
    "    - May or may not want to remove duplicate rows with df.drop_duplicates()\n",
    "        * Can see the duplicates first with df.duplicated() (and tack on .sum() for total number)\n",
    "    - Look at how sparse the data is (how many null values for certain columns) with df.isnull().sum(). \n",
    "        * Be careful with removing null values.\n",
    "        * Probably more important to think about removing null values in a data science context instead of a data analysis context\n",
    "        * Null values can be missing at random or systematically/not randomly. When they are not random they might provide valuable information. There might be reasons for missing values that we want to know, or trends in when the data is missing that provide insights\n",
    "        * Some ways to handle it (will depend on the situation) : deleting rows with missing values (used more often when there aren't a lot of missing values and there is a lot of data so it won't be as missed), filling them in with likely values (regression or average or most common value), \n",
    "    - Will eventually want to plot things and look at distributions and that kind of thing but we aren't focusing on visualizations right now\n",
    "    - For categorical data, look at how many different categories there are - use df.nunique() and df.unique() and df['column_name'].value_counts()\n",
    "    - Look at some basic statistics about the data - can use df.describe()\n",
    "    \n",
    "Outliers\n",
    "    - 3 different types:\n",
    "        * Global Outliers (aka Point Anomalies) - its value is far outside the entirety of the data set in which it is found\n",
    "        * Contextual Outliers (aka Conditional Anomalies) - data points whose value significantly deviates from other data within the same context (quantity of time). e.g. abnormal compared to the seasonal pattern\n",
    "        * Collective Outliers - when a subset of data points as a collection deviate significantly from the entire data set \n",
    "        \n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
